## Отчёт по проекту «RAG-ассистент Сбербанка (мониторинг и QA)»

### Описание проекта

- **Название проекта**: RAG-ассистент Сбербанка (модуль мониторинга и оценки качества, `06-monitoring-qa`)
- **Краткое описание**: Telegram-бот с RAG (Retrieval-Augmented Generation) на базе LangChain, который отвечает на вопросы по документам Сбербанка о кредитах и вкладах, а также поддерживает мониторинг и оценку качества через LangSmith и метрики RAGAS.

### Вариант задания

- **Вариант**: базовый

### Используемые модели и провайдеры

- **Провайдер LLM / embeddings**: OpenRouter (`OPENAI_BASE_URL=https://openrouter.ai/api/v1`)
- **Для основного RAG-потока**:
  - **MODEL**: `openai/gpt-oss-20b:free`
  - **MODEL_QUERY_TRANSFORM**: `openai/gpt-oss-20b:free`
  - **EMBEDDING_MODEL**: `openai/text-embedding-3-large`
- **Для RAGAS evaluation**:
  - **RAGAS_LLM_MODEL**: `openai/gpt-oss-20b:free`
  - **RAGAS_EMBEDDING_MODEL**: `openai/text-embedding-3-large`

---

## Создание и загрузка датасета

### Подход к созданию датасета

- **Инструмент**: модуль `dataset_synthesizer.py` и команды `make dataset` / `make dataset-upload`.
- **Общий пайплайн в коде**:
  - загрузка и разбиение PDF-документов на чанки;
  - автоматический синтез Q&A пар из чанков через LLM;
  - загрузка готовых Q&A пар из JSON-файлов;
  - объединение всех примеров и сохранение в `datasets/06-rag-qa-dataset.json`;
  - опциональная загрузка датасета в LangSmith.
- **Фактическое содержимое текущего датасета** `datasets/06-rag-qa-dataset.json`:
  - 2 Q&A пары, обе импортированы из `sberbank_help_documents.json` (тип `from_json`, категория «Вопросы о дебетовых картах»).

### Размер датасета

- **Количество примеров**: 2

### Скриншот страницы датасета в LangSmith

- **Скриншот**: [Страница датасета в LangSmith](screenshots/LangSmith_dataset.jpg)

### Примеры Q&A пар из датасета

- **Пример 1**
  - **question**: «Какие реквизиты изменятся после перевыпуска?»
  - **ground_truth**:  
    «После перевыпуска изменится:Номер картыCVC-кодСрок действия картыНомер счёта останется прежним. Если вы получаете на карту выплаты, после перевыпуска не нужно сообщать новые данные в бухгалтерию. Автоплатежи и платежи по кредиту в Сбере также будут списываться автоматически.А вот если ваша карта привязана к сервисам бесконтактной оплаты (Mir Pay, Samsung Pay), онлайн-сервисам с регулярным списанием (например, к онлайн-кинотеатру) или приложениям вроде заказа такси, не забудьте обновить номер своей карты в их настройках оплаты.»
  - **contexts[0]**: тот же текст, дополненный заголовком категории и формулировкой вопроса.
  - **metadata**:
    - `source`: `sberbank_help_documents.json`
    - `type`: `from_json`
    - `category`: «Вопросы о дебетовых картах»
    - `url`: `https://www.sberbank.ru/ru/person/help/debet_cards_faq/7902?question=38024`

- **Пример 2**
  - **question**: «Что такое перевыпуск карты?»
  - **ground_truth**:  
    «Перевыпуск — замена вашей текущей карты на новую, но с тем же счётом. После перевыпуска старую карту блокируют, остаётся только новая.»
  - **contexts[0]**: тот же текст, дополненный заголовком категории и формулировкой вопроса.
  - **metadata**:
    - `source`: `sberbank_help_documents.json`
    - `type`: `from_json`
    - `category`: «Вопросы о дебетовых картах»
    - `url`: `https://www.sberbank.ru/ru/person/help/debet_cards_faq/7902?question=38005`

---

## Оценка качества через RAGAS

### Используемые метрики

- **Faithfulness (Обоснованность)**: ответ не содержит галлюцинаций и основан только на retrieved документах.
- **Answer Relevancy (Релевантность ответа)**: насколько ответ релевантен заданному вопросу.
- **Answer Correctness (Правильность)**: насколько ответ соответствует эталонному `ground_truth`.
- **Answer Similarity (Похожесть на эталон)**: семантическая похожесть ответа на эталон.
- **Context Recall (Полнота контекста)**: покрывает ли извлечённый контекст информацию, необходимую для правильного ответа.
- **Context Precision (Точность поиска)**: насколько извлечённый контекст состоит из действительно релевантных документов.

### Фактические результаты запусков evaluation

- **Первый запуск `/evaluate_dataset`**:
  - **Faithfulness (Обоснованность)**: 1.000
  - **Answer Relevancy (Релевантность ответа)**: 0.024
  - **Answer Correctness (Правильность)**: 0.936
  - **Answer Similarity (Похожесть на эталон)**: 0.893
- **Второй запуск `/evaluate_dataset` (c ошибками 429 Rate Limit)**:
  - **Faithfulness (Обоснованность)**: 0.786
  - **Answer Relevancy (Релевантность ответа)**: 0.004
  - **Answer Correctness (Правильность)**: `nan`
  - **Answer Similarity (Похожесть на эталон)**: 0.883
  - **Context Recall (Полнота контекста)**: 1.000
  - **Context Precision (Точность поиска)**: 1.000
- **Скриншоты evaluation и ошибок**:
  - [Результаты первого запуска `/evaluate_dataset`](screenshots/evaluate_dataset.jpg)
  - [Результаты после ошибок и `nan` в метриках](screenshots/evaluate_dataset_after_errors.jpg)
  - [Логи ошибок `429 Rate limit exceeded`](screenshots/logs_errors_429.jpg)

---

## Выводы о качестве RAG-системы

- **Высокая обоснованность и корректность**: значения `faithfulness` (до 1.000) и `answer_correctness` (до 0.936), а также высокая `answer_similarity` (≈0.89) показывают, что модель в целом опирается на корректный контекст и часто даёт ответы, близкие к эталонным.
- **Проблемы с релевантностью формулировки ответа**: очень низкая `answer_relevancy` (0.024 и 0.004) означает, что с точки зрения метрики ответы слабо соответствуют самим формулировкам вопросов (например, могут быть слишком длинными выдержками из текста или не фокусироваться на самом вопросе).
- **Качество поиска контекста высокое**: `context_recall` и `context_precision` на уровне 1.000 во втором запуске указывают, что retriever находит нужные фрагменты документов и почти не добавляет нерелевантный шум.
- **Влияние ограничений провайдера**: ошибки `429 Rate limit exceeded` во втором запуске привели к `nan` в части метрик и ухудшению итоговых значений, поэтому для стабильной оценки качества требуется либо платный тариф/запас квоты у провайдера, либо более экономный режим вызовов evaluation.


