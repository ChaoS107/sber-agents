## Отчёт о выполнении задания по модулю M04 «Мультимодальные агенты»

### Название проекта и краткое описание

**Название проекта:** Персональный финансовый советник (мультимодальный Telegram‑бот).  
Бот помогает пользователю вести учёт доходов и расходов: извлекает транзакции из текстовых сообщений и изображений чеков/скриншотов, сохраняет их в памяти и по запросу показывает баланс и статистику.

### Вариант задания

- **Вариант:** базовый (мультимодальный Telegram‑бот с поддержкой текста и изображений, интеграция с внешними и локальными моделями через OpenRouter и Ollama).

### Реализованные возможности

- [x] Каркас Telegram‑бота на `aiogram` 3.x и запуск через `polling`
- [x] Извлечение транзакций из текстовых сообщений с помощью LLM и structured output (Pydantic модели)
- [x] Обработка изображений чеков и скриншотов через VLM (vision‑модель) и извлечение агрегированных транзакций
- [x] Хранение истории диалога и транзакций в памяти процесса (`chat_conversations`, `transactions`)
- [x] Команды `/start`, `/balance`, `/transactions` с форматированными отчётами и статусами по транзакциям
- [x] Поддержка как облачных моделей через OpenRouter, так и локальных моделей через Ollama (единый клиент `openai`)
- [x] Настройка системных промптов для текста и изображений через файлы в `prompts/` и переменные окружения
- [x] Обработка ошибок при вызове LLM (включая vision) с логированием и нейтральным ответом пользователю
- [ ] Транскрибация голосовых сообщений (запланирована как «Итерация 6», архитектурно описана, но пока не реализована в коде)

### Технологический стек

- **Язык и окружение:** Python 3.11, управление зависимостями через `uv`
- **Telegram:** `aiogram` 3.x (polling), роутер и хендлеры в `handlers.py`
- **LLM‑интеграция:** официальный клиент `openai` (`AsyncOpenAI`) с настраиваемым `base_url`
- **Модели (облако, через OpenRouter):**
  - Текст: `openai/gpt-oss-20b:free` (пример базовой модели для structured output)
  - Изображения: `meta-llama/llama-3.2-11b-vision-instruct` (использовалась на ранних этапах для vision‑функций)
- **Модели (локально, через Ollama):**
  - Текст: `llama3.2`
  - Изображения: `qwen2.5vl:7b` (основная локальная VLM для распознавания русских чеков)
- **Данные и валидация:** `pydantic` (модели `Transaction`, `TransactionResponse`)
- **Конфигурация:** `.env` + модуль `config.py` (чтение токенов, URL, имён моделей и промптов)
- **Оркестрация и запуск:** `Makefile` (`make install`, `make run`)
- **Логирование:** стандартный `logging` (уровень `INFO`, вывод в stdout)

### Инструменты AI-driven разработки

- **IDE с AI‑ассистентом:** Cursor (автодополнение, навигация по коду, рефакторинг с участием LLM, inline‑подсказки).
- **Модели LLM, использованные в процессе разработки:**
  - Семейство GPT (в том числе GPT‑5.1 в составе Cursor) — генерация и правка кода, документации, промптов.
  - OpenRouter‑модели (например, `openai/gpt-oss-20b:free`, multimodal Llama‑vision) — отладка structured output и промптов для чеков.
  - Локальные модели Ollama (llama3.2 / llama3.2‑vision) — экспериментальная проверка работы бота без внешнего интернета.

### Скриншоты работы бота

- [Текстовый расход (пример)](screenshots/transaction_text_exp.jpg)
- [Текстовый доход (пример)](screenshots/transaction_text_inc.jpg)
- [Обработка изображения чека / скриншота](screenshots/transaction_image_exp.jpg)

### Облачный сервер / окружение развёртывания

- **Облачный сервер:** в рамках задания отдельный облачный сервер не поднимался — бот и модели тестировались локально на рабочем ПК (Windows 10).  
- **Провайдер LLM в облаке:** OpenRouter (через HTTP‑API, `OPENAI_BASE_URL=https://openrouter.ai/api/v1`).  
- **Локальные модели Ollama:** запускались локально, без отдельного GPU‑облака:
  - `llama3.2` для текста;
  - `llama3.2-vision` для изображений чеков.  
- **GPU:** для экспериментов с Ollama использовался локальный GPU (NVIDIA), настройка происходила автоматически со стороны Ollama; при необходимости можно управлять количеством слоёв на GPU через переменную `OLLAMA_GPU_LAYERS`.

### Основные вызовы и решения

- **Мультимодальная интеграция через единый клиент `openai`.**  
  Нужно было поддержать и OpenRouter, и Ollama, не плодя обёрток. Это решено через один `AsyncOpenAI` с конфигурацией `OPENAI_BASE_URL` и `MODEL_TEXT` / `MODEL_IMAGE` из `.env`, что упростило переключение провайдеров.
- **Structured output для изображений чеков.**  
  Vision‑модели часто возвращают «текст с комментарием», а не строгий JSON. Для надёжности добавлена валидация через Pydantic, явное логирование «сырого» ответа и обработка случаев, когда `transactions` или `answer` отсутствуют.
- **Качество распознавания чеков.**  
  Первые версии промпта часто возвращали пустой список транзакций. Проблема смягчена за счёт усиления системного промпта (обязательное извлечение транзакций при наличии сумм, акцент на поле «ИТОГ») и подбора подходящих vision‑моделей.
- **Баланс KISS/YAGNI и мультимодальности.**  
  Несмотря на текст+изображения (и планируемый голос), архитектура сохранена максимально простой: один модуль `handlers.py`, один `llm.py`, минимум вспомогательных слоёв, хранение данных только в памяти.
- **Обработка ошибок и диагностика.**  
  Vision‑запросы могут падать по причинам модели/провайдера, поэтому вокруг вызовов LLM добавлены `try/except`, логирование ошибок и информативные сообщения пользователю без раскрытия технических деталей.

### Что я узнал нового

1. **Как строить единый слой интеграции для разных провайдеров (OpenRouter, Ollama) на базе клиента `openai`, чтобы менять модели конфигурацией, а не переписывать код.**
2. **Особенности промпт‑инжиниринга для VLM: чтобы извлекать транзакции из чеков, нужно явно акцентировать суммы, итоговые поля и формат JSON, иначе модель легко «забывает» про структуру.**
3. **Практика использования Pydantic и JSON Schema с `response_format` для строгого structured output заметно повышает надёжность по сравнению с «свободным» текстовым ответом.**
4. **При работе с мультимодальными LLM особенно важны детальные логи «сырых» ответов — без них трудно понять, где проблема: в модели, промпте или парсинге.**
5. **AI‑ассистент в IDE (Cursor) хорошо помогает не только писать код, но и поддерживать целостность документации (`vision.md`, `tasklist.md`, промптов и отчёта), что упрощает итеративную AI‑driven разработку.**


