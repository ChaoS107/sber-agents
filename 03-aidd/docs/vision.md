1. **Технологии**

- **Язык и окружение**
  - Python 3.11 как основная версия интерпретатора
  - `uv` для управления зависимостями и запуска команд проекта

- **Работа с LLM**
  - Клиент `openai` для обращения к провайдеру OpenRouter через OpenAI-совместимый HTTP API
  - Только текстовый режим (чат-модель) на этапе MVP, без мультимодальности

- **Интеграция с Telegram**
  - Фреймворк `aiogram` (v3) для работы с Telegram Bot API
  - Получение обновлений только через `polling` (без вебхуков) для упрощения запуска и отладки

- **Оркестрация и сборка**
  - `make` для типовых команд разработки и запуска (например, `make init`, `make run-bot`)

- **Конфигурация и среда выполнения**
  - Файл `.env` для хранения чувствительных и средозависимых настроек (токены, ключи, базовые параметры)
  - Простейший самописный загрузчик конфигурации на Python без дополнительных библиотек

- **Хранение данных**
  - Только оперативная память процесса (in-memory структуры данных), без использования БД
  - Осознанное допущение на этапе проверки гипотезы: данные (история, маршруты) теряются при перезапуске бота

- **Логирование**
  - Стандартный модуль `logging` из стандартной библиотеки Python
  - Логирование в stdout с простым текстовым форматом без сторонних решений и сервисов на этапе MVP

2. **Принципы разработки**

- **KISS и YAGNI**
  - Реализуем только то, что необходимо для проверки гипотезы: один Telegram-бот, базовый диалог и интеграция с LLM.
  - Избегаем абстракций и слоёв «на будущее», добавляем новые элементы только при появлении конкретной потребности.

- **MVP-ориентированность**
  - Главная цель — как можно быстрее запустить работающего бота, который принимает сообщения, обращается к LLM с фиксированным системным промптом и возвращает ответы туристического консультанта.
  - Дополнительные функции (расширенные сценарии, интеграции, долгосрочное хранение данных, аналитика) откладываются до подтверждения ценности MVP.

- **Ясная и простая структура**
  - Проект делится на небольшое число зон ответственности: Telegram-слой, работа с LLM и простая доменная логика.
  - Используем один основной исполняемый модуль для запуска бота без лишних фреймворков поверх `aiogram`.

- **Простота развёртывания**
  - Развёртывание и запуск должны укладываться в несколько понятных команд (`uv` для зависимостей, `make` для запуска).
  - Без Docker, сложного CI/CD и внешней инфраструктуры на этапе MVP.

- **Прозрачность и отладка**
  - Логи должны позволять отследить ключевые шаги: входящие сообщения, запросы к LLM, ответы и ошибки.
  - Локальный запуск с тестовым токеном Telegram и ключом OpenRouter должен быть максимально простым.

3. **Структура проекта**

- **Корень репозитория**
  - `pyproject.toml` — описание проекта и зависимостей для `uv`.
  - `Makefile` — основные команды разработки и запуска (инициализация зависимостей, запуск бота).
  - `README.md` — краткое описание проекта и инструкции по установке/запуску.
  - `docs/` — документация проекта, включая `idea.md` и `vision.md`.

- **Исходный код**
  - `src/`
    - `bot.py` — точка входа приложения: инициализация `aiogram`, настройка логирования, регистрация обработчиков и запуск `polling`; базовая логика обработки сообщений и вызова LLM.
    - `config.py` — минимальная логика загрузки конфигурации из `.env` и предоставление настроек (токены, ключи, базовые параметры).
    - `llm.py` — тонкая обёртка над клиентом `openai` для отправки запросов в OpenRouter с учётом системного промпта и параметров модели.

На этапе MVP вся основная логика (обработка входящих сообщений, вызовы LLM, базовая обработка ошибок и логирование) сосредоточена в файле `bot.py`, без дополнительных подпакетов и модулей.

4. **Архитектура проекта**

- **Основные компоненты**
  - Telegram-слой (`aiogram` в `bot.py`):
    - инициализация бота и диспетчера;
    - регистрация простого хендлера текстовых сообщений;
    - запуск `polling`.
  - LLM-слой (`llm.py`):
    - инициализация клиента `openai` с ключом OpenRouter;
    - одна простая функция для обращения к модели (например, `ask_llm`), которая:
      - собирает список сообщений для LLM (системный промпт + контекст диалога + последнее сообщение пользователя),
      - вызывает чат-модель,
      - возвращает текст ответа.
  - Конфигурация (`config.py`):
    - загрузка переменных окружения из `.env` (токены, ключи, базовые параметры моделей);
    - предоставление настроек в виде простых атрибутов/констант.

- **Поток данных**
  - Пользователь отправляет сообщение в Telegram-бот.
  - `aiogram` вызывает хендлер в `bot.py`.
  - Хендлер:
    - логирует входящее сообщение;
    - обновляет in-memory контекст диалога по `user_id` (например, хранит последние N реплик);
    - формирует параметры вызова LLM и передаёт текст + контекст в функцию из `llm.py`;
    - получает ответ от LLM и отправляет его пользователю.

- **Состояние и контекст диалога**
  - Для каждого пользователя в памяти процесса хранится небольшой контекст диалога (например, несколько последних обменов сообщениями).
  - Контекст хранится в простой структуре данных (словарь `user_id -> список сообщений`) внутри `bot.py`.
  - При перезапуске бота контекст теряется, что является допустимым ограничением для MVP.

- **Обработка ошибок**
  - В хендлере сообщений (`bot.py`) вызовы к LLM обёрнуты в `try/except`.
  - В случае ошибки:
    - ошибка логируется (с трассировкой стека);
    - пользователю возвращается короткое нейтральное сообщение в духе «Сервис временно недоступен, попробуйте позже».

5. **Модель данных**

- **Пользователь**
  - Идентифицируется по `user_id` из Telegram (целое число или строка в зависимости от формата, возвращаемого `aiogram`).
  - Дополнительные поля пользователя на этапе MVP явно не хранятся (используются только при необходимости в логах).

- **Сообщение диалога**
  - Представляется простым словарём с полями:
    - `role: str` — роль в диалоге с LLM (`"system"`, `"user"`, `"assistant"`).
    - `content: str` — текст сообщения.
  - Эти структуры используются как для локального контекста, так и для подготовки списка сообщений при вызове LLM.

- **Контекст диалога**
  - В памяти процесса хранится одна общая структура:
    - `dialog_context: dict[user_id, list[message]]`
    - где `user_id` — идентификатор пользователя Telegram,
    - `message` — словарь вида `{ "role": "...", "content": "..." }`.
  - Для каждого пользователя хранится только ограниченное число последних сообщений (например, 6–10), старые сообщения постепенно отбрасываются.
  - Контекст существует только в оперативной памяти и теряется при перезапуске бота.

6. **Работа с LLM**

- **Провайдер и модель**
  - Взаимодействие с LLM осуществляется через провайдера OpenRouter по OpenAI-совместимому API с использованием клиента `openai`.
  - Базовый URL API и идентификатор модели читаются из переменных окружения `.env` (например, `OPENROUTER_BASE_URL` и `MODEL_NAME`), без жёсткого захардкоживания в коде.

- **Системный промпт**
  - Роль ассистента задаётся фиксированным системным промптом, описывающим бота как профессионального туристического консультанта.
  - Системный промпт хранится в модуле `llm.py` в виде константы (многострочная строка), не выносится в конфигурацию на этапе MVP.

- **Формирование запросов к модели**
  - Модуль `llm.py` предоставляет одну основную функцию (например, `ask_llm(user_id: str, user_message: str, history: list[dict]) -> str`), которая:
    - принимает идентификатор пользователя, текст текущего сообщения и список последних сообщений диалога;
    - формирует список сообщений для LLM в формате:
      - одно системное сообщение (`role="system"`, `content=<системный промпт>`),
      - несколько последних сообщений истории (`role="user"/"assistant"`),
      - текущее сообщение пользователя (`role="user"`).
  - Формирование списка сообщений происходит максимально прямолинейно, без дополнительной логики обработки текста.

- **Параметры генерации**
  - В `llm.py` задаются простые константы для параметров модели:
    - `TEMPERATURE` (например, `0.7`),
    - `MAX_TOKENS` (например, `512`).
  - Остальные параметры (top_p, frequency_penalty и т.п.) остаются значениями по умолчанию клиента и явно не настраиваются на этапе MVP.

- **Ограничение контекста**
  - Перед формированием запроса к LLM история диалога ограничивается небольшим числом последних сообщений (например, 6–10), более старые сообщения отбрасываются.
  - Управление длиной истории осуществляется простым срезом списка без подсчёта токенов.

7. **Сценарии работы**

- **Сценарий 1: Старт работы с ботом (`/start`)**
  - Пользователь отправляет команду `/start`.
  - Бот:
    - создаёт (или очищает) контекст диалога для данного `user_id`;
    - отправляет короткое приветствие с объяснением роли (профессиональный туристический консультант);
    - даёт 1–2 примера вопросов (например, про выбор направления и составление маршрута).

- **Сценарий 2: Получение справки (`/help`)**
  - Пользователь отправляет команду `/help`.
  - Бот отправляет краткое описание возможностей:
    - какие вопросы можно задавать (подбор направления, маршрута, отелей, сезонов, виз и т.п.);
    - как лучше формулировать запрос (указать даты, бюджет, предпочтения);
    - упоминание того, что бот не бронирует билеты/отели, а даёт рекомендации.

- **Сценарий 3: Обычный диалог (текстовые сообщения)**
  - Пользователь отправляет произвольное текстовое сообщение (не команду).
  - Бот:
    - логирует сообщение;
    - обновляет in-memory контекст диалога для `user_id` (добавляет новое сообщение, при необходимости обрезает историю до последних 6–10 сообщений);
    - вызывает функцию `ask_llm(...)` с системным промптом, историей и текущим сообщением;
    - отправляет пользователю ответ LLM в виде одного текстового сообщения.

- **Сценарий 4: Сброс диалога (`/reset`)**
  - Пользователь отправляет команду `/reset`.
  - Бот:
    - удаляет (очищает) контекст диалога для текущего `user_id` из `dialog_context`;
    - отправляет короткое подтверждение (например, «История диалога очищена, начнём сначала»).

- **Сценарий 5: Ошибки при обращении к LLM или сети**
  - При любой ошибке при вызове LLM или сетевой ошибке:
    - ошибка логируется с трассировкой стека;
    - пользователю отправляется короткое нейтральное сообщение: «Сервис временно недоступен, попробуйте позже».

8. **Подход к конфигурированию**

- **Источник конфигурации**
  - Все чувствительные и средозависимые параметры задаются через переменные окружения, как правило, с использованием файла `.env`.
  - Загрузка `.env` осуществляется внешними средствами (настройки среды, IDE, shell), а приложение читает значения через `os.environ` без дополнительных библиотек.

- **Модуль конфигурации (`config.py`)**
  - Отвечает за чтение необходимых переменных окружения и предоставление их в виде простых констант/атрибутов.
  - На этапе MVP используется минимальная проверка наличия критичных переменных (например, выбрасывание исключения при отсутствии токена бота или ключа LLM).

- **Основные переменные окружения**
  - `TELEGRAM_BOT_TOKEN` — токен Telegram-бота.
  - `OPENROUTER_API_KEY` — ключ доступа к OpenRouter.
  - `OPENROUTER_BASE_URL` — базовый URL для обращения к API OpenRouter.
  - `MODEL_NAME` — идентификатор используемой LLM-модели (может меняться без правки кодовой базы).
  - `LOG_LEVEL` (опционально) — уровень логирования (по умолчанию `INFO`, если переменная не задана).

9. **Подход к логгированию**

- **Базовая настройка**
  - Логирование настраивается в `bot.py` с помощью стандартного модуля `logging` и `logging.basicConfig`.
  - Уровень логирования берётся из переменной окружения `LOG_LEVEL` (по умолчанию `INFO`, если не задана).
  - Формат логов: время, уровень, имя логгера и сообщение (например, `%(asctime)s [%(levelname)s] %(name)s: %(message)s`).
  - Логи выводятся только в stdout, без записи в файлы и без внешних лог-сервисов.

- **Что логируем**
  - Факт запуска бота и успешной инициализации (`bot.py`).
  - Входящие сообщения пользователей (идентификатор пользователя + короткий текст сообщения).
  - Обращения к LLM (идентификатор пользователя, имя модели, длина или обрезанный текст запроса и ответа).
  - Ошибки при работе с Telegram API, LLM и сетью (с трассировкой стека на уровне `ERROR`).

- **Что не логируем**
  - Не логируем в явном виде чувствительные данные (токены, ключи доступа).
  - Не используем отдельные файлы логов или ротацию на этапе MVP, чтобы не усложнять инфраструктуру.
