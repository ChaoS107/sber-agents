## RAG-ассистент Сбербанка

Телеграм‑бот с RAG (Retrieval‑Augmented Generation) для ответов на вопросы по продуктам Сбербанка (кредиты, вклады, карты) на основе внутренних документов и FAQ.

---

## Вариант задания

- **Вариант:** расширенный (RAG‑ассистент с PDF + JSON датасетом и сравнением моделей эмбеддингов)

---

## Реализованные возможности

- [x] **Телеграм‑бот на aiogram 3.x** с базовыми командами (`/start`, `/help`, `/index`, `/index_status`)
- [x] **RAG на базе LangChain** (retriever + генерация ответа с учётом контекста)
- [x] **Индексация PDF‑документов** из директории `data/` при старте бота
- [x] **Индексация JSON‑датасета вопросов‑ответов** (`sberbank_help_documents.json`) через `JSONLoader` и `jq_schema`
- [x] **Единое векторное хранилище** для PDF и JSON‑данных в памяти процесса (`InMemoryVectorStore`)
- [x] **Контекстный диалог** с учётом истории сообщений и query‑transformation
- [x] **Query Transformation цепочка** для улучшения поискового запроса на основе истории диалога
- [x] **Логирование в файл и консоль** (запуск бота, индексация, ошибки)
- [x] **Конфигурация через `.env`** (токены, базовый URL, модели, директории данных и промптов)
- [x] **Эксперименты с параметрами индексации** (размер чанка, overlap, `k` для retriever)
- [x] **Сравнение моделей эмбеддингов** (OpenRouter/OpenAI vs Fireworks) для русскоязычных банковских документов

---

## Технологический стек

- **Язык и окружение**
  - **Python 3.11+**
  - **uv** — управление зависимостями и виртуальным окружением

- **Telegram и бэкенд**
  - **aiogram 3.x** — обработка команд и сообщений
  - **logging (stdlib)** — базовое логирование

- **RAG и работа с документами**
  - **LangChain / langchain-core / langchain-community**
  - **langchain-openai** — ChatOpenAI и OpenAIEmbeddings
  - **PyPDFLoader (langchain_community.document_loaders)** — загрузка PDF
  - **RecursiveCharacterTextSplitter** — разбиение на текстовые чанки
  - **InMemoryVectorStore** — in‑memory векторное хранилище
  - **JSONLoader + jq** — загрузка и парсинг JSON‑датасета

- **Конфигурация и утилиты**
  - **python-dotenv** — загрузка `.env`
  - **Makefile** — команды `make install`, `make run`

---

## Используемые модели

- **Основная LLM (через OpenAI‑совместимый API)**
  - Вариант OpenRouter:
    - `MODEL=openai/gpt-oss-20b:free`
    - `MODEL_QUERY_TRANSFORM=openai/gpt-oss-20b:free`
  - Вариант Fireworks:
    - `MODEL=accounts/fireworks/models/gpt-oss-120b`
    - `MODEL_QUERY_TRANSFORM=accounts/fireworks/models/gpt-oss-120b`

- **Модели эмбеддингов**
  - Вариант OpenRouter/OpenAI:
    - `EMBEDDING_MODEL=openai/text-embedding-3-large`
  - Вариант Fireworks:
    - `EMBEDDING_MODEL=accounts/fireworks/models/qwen3-embedding-8b`

---

## Эксперименты с индексацией (размеры чанков)

### Параметры, которые пробовали

Тестировали несколько сочетаний параметров `chunk_size` и `chunk_overlap` в `RecursiveCharacterTextSplitter`:

- **Вариант A (мелкие чанки):**
  - `chunk_size = 500`
  - `chunk_overlap = 50`
- **Вариант B (средние чанки):**
  - `chunk_size = 1000`
  - `chunk_overlap = 100`
- **Вариант C (крупные чанки, текущий):**
  - `chunk_size = 1500`
  - `chunk_overlap = 150`

Также варьировали количество возвращаемых документов retriever:

- `RETRIEVER_K = 3` и `RETRIEVER_K = 5`

### Наблюдения

- **Мелкие чанки (500/50)**
  - Плюсы:
    - Точный таргетинг на короткие фрагменты текста.
    - Меньше «лишнего шума» в контексте LLM.
  - Минусы:
    - Для юридических и продуктовых описаний Сбера один абзац часто не замыкает мысль.
    - Часто приходилось подтягивать 4–5 чанков, чтобы покрыть весь ответ.
    - Рост количества запросов к векторному хранилищу и увеличение латентности.

- **Средние чанки (1000/100)**
  - Плюсы:
    - Уменьшилось число случаев, когда нужная норма «разорвана» между чанками.
    - Чаще один‑два чанка уже содержат всю необходимую информацию.
  - Минусы:
    - Для длинных разделов (например, полный перечень условий по вкладам) всё ещё бывает разброс информации по 2–3 чанкaм.

- **Крупные чанки (1500/150)**
  - Плюсы:
    - Для типичных банковских документов (длинные формализованные разделы) один чанк почти всегда покрывает всю норму целиком.
    - Ответы на вопросы про ставки, комиссии и условия реже оказываются «обрезанными».
    - Особенно хорошо работает для вопросов типа «какие условия по кредиту Х» или «какие комиссии по карте Y».
  - Минусы:
    - Менее точная локализация небольших деталей: если пользователь спрашивает про очень частный случай, в чанк попадает больше лишнего текста.
    - Чуть выше стоимость эмбеддингов и время индексации.

- **RETRIEVER_K = 3 vs 5**
  - При `k=5` в ответ начинало попадать слишком много контекста, ответы становились более «водянистыми».
  - `k=3` дало лучший баланс между полнотой и фокусом для русскоязычных банковских документов.

### Выводы по стратегии индексации для банковских документов

- Для длинных и формализованных банковских документов (условия кредитов, вклады, тарифы по картам) **крупные чанки с лёгким overlap** показали себя лучше:
  - **Оптимальная конфигурация для этого проекта:**  
    - `chunk_size = 1500`  
    - `chunk_overlap = 150`  
    - `RETRIEVER_K = 3`
- Такая стратегия:
  - Снижает вероятность потери контекста при ответах на комплексные вопросы.
  - Даёт LLM достаточно «самодостаточный» фрагмент для генерации точного и юридически корректного ответа.

---

## Работа с JSON‑датасетом

### Формат датасета

- Файл: `data/sberbank_help_documents.json`
- Структура: массив объектов, каждый из которых содержит подготовленный текстовый фрагмент FAQ/справки банка.
- В коде используется поле `full_text`, где собраны вопросы и ответы в удобном для RAG виде.

### Загрузка JSON (JSONLoader + jq)

Для загрузки JSON‑датасета реализована функция `load_json_documents` в `src/indexer_with_json.py`, использующая `JSONLoader` из `langchain_community` и `jq_schema`:

- **Ключевые идеи реализации:**
  - Проверка наличия файла по пути, логирование предупреждений, если файл отсутствует.
  - Использование `jq_schema` для извлечения нужного поля из каждого элемента массива.
  - Преобразование JSON‑данных в список документов LangChain и последующее объединение с PDF‑чанками.

**Используемая схема:**

- `jq_schema = '.[].full_text'` — берём поле `full_text` для каждого элемента корневого массива.

JSON‑документы затем объединяются с PDF‑чанками:

- `all_chunks = pdf_chunks + json_chunks`
- Индексация идёт в общее `InMemoryVectorStore`, поэтому бот прозрачно отвечает как по PDF, так и по JSON‑FAQ.

### Скриншот работы с вопросами про карты

Пример диалога, где пользователь задаёт вопросы про дебетовые и кредитные карты, а ответы приходят из JSON‑датасета:

- [Диалог с вопросами про карты (скриншот)](screenshots/cards-faq-dialog.png)

---

## Сравнение моделей эмбеддингов

### Протестированные модели

- **OpenRouter / OpenAI:**
  - `openai/text-embedding-3-large`
- **Fireworks:**
  - `accounts/fireworks/models/qwen3-embedding-8b`

### Критерии оценки

- Точность извлечения релевантных фрагментов для русскоязычных запросов:
  - вопросы про условия вкладов;
  - вопросы про комиссии и лимиты по картам;
  - вопросы про реструктуризацию и досрочное погашение кредитов.
- Количество случаев, когда бот отвечает «Я не нашёл ответа…» при наличии релевантного текста в документах.
- Стабильность качества при переформулировке вопросов (синонимы, разговорный язык).

### Наблюдения

- **`text-embedding-3-large` (OpenRouter/OpenAI)**
  - Очень хорошее качество на формальных формулировках из PDF‑документов.
  - Хорошо справляется с длинными вопросами и сложными юридическими конструкциями.
  - Иногда хуже ловит разговорные/сокращённые формулировки и банковский жаргон.

- **`qwen3-embedding-8b` (Fireworks)**
  - Лучше справляется с вариативностью русскоязычных запросов (синонимы, разговорный стиль).
  - Чуть лучше на вопросах, связанных с «человеческими» формулировками про карты и кредиты («можно ли снять без комиссии», «что будет, если…»).
  - На строго формализованных вопросах (дословные цитаты из документа) качество сопоставимо с `text-embedding-3-large`.

### Выводы: какая модель лучше для русского языка

- Для русскоязычных банковских запросов с живым языком и разнообразными формулировками **лучшую восприимчивость показала**:
  - **`accounts/fireworks/models/qwen3-embedding-8b` (Fireworks)** — лучше ловит синонимы и разговорные варианты.
- Для максимально формальных юридических формулировок разница с `text-embedding-3-large` минимальна, обе модели дают хорошее качество.
- В результате:
  - **Рекомендация для русскоязычного банковского ассистента:** использовать Fireworks Qwen‑эмбеддинги как основной вариант,  
    а `text-embedding-3-large` рассматривать как надёжный fallback/альтернативу, если важнее интеграция через OpenRouter/OpenAI.

---

## Скриншоты

- [Диалог с вопросами про карты](screenshots/cards-faq-dialog.png)
- [Пример ответа по условиям вклада](screenshots/deposit-conditions-dialog.png)
- [Пример ответа по потребительскому кредиту](screenshots/consumer-credit-dialog.png)
- [Логи успешной индексации PDF + JSON](screenshots/indexing-logs.png)


